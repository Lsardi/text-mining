{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des librairie necessaire\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ouvrirFichier(non_fichier):\n",
    "    '''\n",
    "    Objectif : Ouvre un fichier dont le chemin est passé en paramètre et renvoie son contenu\n",
    "    \n",
    "    Entrée :\n",
    "    - chemin du fichier\n",
    "    \n",
    "    Sortie :\n",
    "    - le contenue du fichier\n",
    "    '''\n",
    "    \n",
    "    fichier = open(non_fichier, \"r\")\n",
    "    contenu = fichier.read()\n",
    "    return contenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concatDataset(path_dossier,nomFichier, extension = \".txt\"):\n",
    "    '''\n",
    "    Objectif : Trouve tout les fichier txt d'un dossier donnéselon l'extension passer en paramètre et les \n",
    "    concatènent dans un seul et meme fichier \n",
    "    \n",
    "    Entrée :\n",
    "    - chemin du dossier\n",
    "    - nom du fichier de sortie\n",
    "    - extension du fichier de sortie [default = \".txt\"]\n",
    "    '''\n",
    "    \n",
    "    listeFichierTrouve = [path_dossier + \"/\" + file for file in os.listdir(path_dossier) if file.endswith(extension)]\n",
    "    \n",
    "    fileComplet = open(nomFichier + extension,'a')\n",
    "    contenu= \"\"\n",
    "\n",
    "    for element in listeFichierTrouve :\n",
    "        fichier = open(element,\"r\")\n",
    "        contenu += fichier.read()\n",
    "        fichier.close()\n",
    "\n",
    "    fileComplet.write(contenu)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformeFileData (contenu, nomDataset) :\n",
    "    '''\n",
    "    Objectif : Compose un dataset au format csv a partir d'un fichier txt non nettoyé\n",
    "    \n",
    "    Entrée :\n",
    "    - contenu du fichier txt\n",
    "    - nom du fichier csv de sortie\n",
    "    \n",
    "    Sortie :\n",
    "    - le dataFrame enregistré dans le fichier csv\n",
    "    '''\n",
    "    \n",
    "    element = contenu.split(\"</pair>\")\n",
    "\n",
    "    list_label =[]\n",
    "    list_attribut =[]\n",
    "\n",
    "    for ligne in (element):\n",
    "        liste = (re.findall(\"entailment=\\\".*\\\"\",ligne))\n",
    "        tex = (re.findall(\"<t>\\n.*\\n</t>\",ligne))\n",
    "\n",
    "        for r in tex:\n",
    "            phrase= re.sub(\"<.*?>\",\"\",r)\n",
    "            phrase= phrase.replace(\"\\n\",\"\")\n",
    "            list_attribut.append(phrase)\n",
    "            #print(phrase)\n",
    "\n",
    "        for mot in liste:\n",
    "            label=(re.findall(\"\\\".*?\\\"\" , mot)[0])\n",
    "            label=label.replace('\"','')\n",
    "            list_label.append(label)\n",
    "            #print(label)\n",
    "\n",
    "\n",
    "    data_frame = pd.DataFrame()\n",
    "\n",
    "    for label, text in zip(list_attribut,list_label):\n",
    "        line = pd.DataFrame([[label, text]])\n",
    "        data_frame = pd.concat([data_frame, line])\n",
    "\n",
    "    data_frame.columns=[\"Text\", \"Label\"]\n",
    "    data_frame = data_frame.reset_index(drop=True)\n",
    "\n",
    "    data_frame.to_csv(nomDataset + \".csv\")\n",
    "    \n",
    "    return data_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatisation(listeMot):\n",
    "    \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    for element in listeMot :\n",
    "        element = lemmatizer.lemmatize(element)\n",
    "\n",
    "    return listeMot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def racinisation(listeMot):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [stemmer.stem(element) for element in listeMot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def tokenization(dataframe, colFeature = [\"Text\"]) :\n",
    "    '''\n",
    "    Objectif : Tokenize les features textuel dun dataFrame\n",
    "    \n",
    "    Entrée :\n",
    "    - le dataFrame dorigine\n",
    "    - la liste des colonnes à tokenizer [default = [\"Text\"]]\n",
    "    \n",
    "    Sortie :\n",
    "    - le contenue du fichier\n",
    "    '''\n",
    "    dataCopie = copy.deepcopy(dataframe)\n",
    "    \n",
    "    for col in colFeature:\n",
    "        for indexLigne in range (0,len(dataCopie[col])):\n",
    "\n",
    "            # remplace le texte par une liste de tokens\n",
    "            featureTokenization = nltk.word_tokenize(dataCopie[col][indexLigne])\n",
    "\n",
    "            # nettoyage de la ponctuation\n",
    "            listePonctuation =  [\",\",\"'\",\".\",\"?\",\"!\",\"''\",\"``\",\")\",\"]\",\"(\",\"[\"]\n",
    "            featureTokenization_clean = [element for element in featureTokenization if element not in listePonctuation]\n",
    "\n",
    "            # lemmatisation\n",
    "            featureLemmatisation = lemmatisation(featureTokenization_clean)\n",
    "\n",
    "            # racinisation\n",
    "            featureRacine = racinisation(featureLemmatisation)\n",
    "\n",
    "            dataCopie.set_value(indexLigne,col,featureRacine)\n",
    "                \n",
    "                \n",
    "    return dataCopie\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/user/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_complet.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-13d972b28a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"he is a boy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset_complet.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mcontenu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcontenu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<.*?>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontenu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_complet.txt'"
     ]
    }
   ],
   "source": [
    "# test Part of speech\n",
    "\n",
    "# ouverture du dataset complet\n",
    "# nettoyage des balise html\n",
    "# tokenisation\n",
    "# tagging\n",
    "# count frequency\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "text = \"he is a boy\"\n",
    "file = open(\"dataset_complet.txt\")\n",
    "contenu = file.read()\n",
    "contenu = re.sub(\"<.*?>\",\"\",contenu)\n",
    "\n",
    "text = nltk.word_tokenize(contenu)\n",
    "tagged_text=nltk.pos_tag(text)\n",
    "\n",
    "\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_text)\n",
    "most = tag_fd.most_common()\n",
    "print(most)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "            current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "\n",
    "my_sent = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "get_continuous_chunks(my_sent)\n",
    "['WASHINGTON', 'New York', 'Loretta E. Lynch', 'Brooklyn']\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text    Label\n",
      "0     [crude, oil, for, april, deliveri, trade, at, ...       NO\n",
      "1     [oracl, had, fought, to, keep, the, form, from...       NO\n",
      "2     [all, genet, modifi, food, includ, soya, or, m...      YES\n",
      "3     [research, at, the, harvard, school, of, publi...       NO\n",
      "4     [eat, lot, of, food, that, are, a, good, sourc...      YES\n",
      "5     [the, yanke, split, hollywood, with, someth, t...      YES\n",
      "6     [scientist, at, the, genom, institut, of, sing...  UNKNOWN\n",
      "7     [phish, disband, after, a, final, concert, in,...  UNKNOWN\n",
      "8     [euro-scandinavian, media, cheer, denmark, v, ...      YES\n",
      "9     [iraqi, milit, said, sunday, they, would, behe...      YES\n",
      "10    [two, turkish, engin, and, an, afghan, transla...      YES\n",
      "11    [If, a, mexican, approach, the, border, he, 's...  UNKNOWN\n",
      "12    [iran, will, soon, releas, eight, british, ser...  UNKNOWN\n",
      "13    [the, wait, time, for, a, green, card, ha, ris...      YES\n",
      "14    [coal, compani, stock, got, a, lift, monday, m...      YES\n",
      "15    [the, royal, navi, servicemen, be, held, capti...      YES\n",
      "16    [total, coal, stock, with, the, thermal, power...      YES\n",
      "17    [anorexia, in, male, account, for, approxim, s...      YES\n",
      "18    [there, are, discuss, in, california, and, ari...       NO\n",
      "19    [south, korea, 's, deputi, foreign, minist, sa...  UNKNOWN\n",
      "20    [the, privat, own, spacecraft, onli, got, abou...       NO\n",
      "21    [the, media, alway, talk, about, the, dow, be,...      YES\n",
      "22    [the, feder, aviat, administr, 's, associ, adm...       NO\n",
      "23    [the, lo, angel, laker, roll, over, the, india...      YES\n",
      "24    [world, health, offici, warn, yesterday, that,...  UNKNOWN\n",
      "25    [the, countri, 's, largest, privat, employ, wa...  UNKNOWN\n",
      "26    [the, last, polio, outbreak, in, the, unit, st...  UNKNOWN\n",
      "27    [the, massachusett, suprem, judici, court, ha,...      YES\n",
      "28    [In, june, the, suprem, court, rule, that, ant...      YES\n",
      "29    [sharon, warn, arafat, could, be, target, for,...  UNKNOWN\n",
      "...                                                 ...      ...\n",
      "4465  [u.s., secretari, of, state, colin, powel, rep...      YES\n",
      "4466  [the, centre-right, european, peopl, 's, parti...      YES\n",
      "4467  [ron, gainsford, chief, execut, of, the, tsi, ...  UNKNOWN\n",
      "4468  [chirac, also, call, on, rich, nation, to, rai...      YES\n",
      "4469  [the, icc, the, first, perman, global, crimin,...       NO\n",
      "4470  [iraq, ha, enter, it, first, full, day, under,...      YES\n",
      "4471  [most, american, are, familiar, with, the, foo...      YES\n",
      "4472  [further, surpris, were, reveal, in, the, spac...      YES\n",
      "4473  [all, but, 11, of, the, 107, patient, and, all...      YES\n",
      "4474  [the, u.s., ambassador, to, manila, franci, ri...       NO\n",
      "4475  [about, 85, percent, of, dane, belong, to, the...       NO\n",
      "4476  [the, suicid, bomb, came, as, the, iraqi, inte...  UNKNOWN\n",
      "4477  [tan, salon, are, increasingli, popular, -, th...      YES\n",
      "4478  [jessica, litman, a, law, professor, at, michi...  UNKNOWN\n",
      "4479  [In, all, militari, polic, have, launch, 93, i...       NO\n",
      "4480  [four, venezuelan, firefight, who, were, trave...  UNKNOWN\n",
      "4481  [the, thick, atmospher, of, titan, make, it, d...       NO\n",
      "4482  [four, venezuelan, firefight, who, were, trave...      YES\n",
      "4483  [A, longtim, associ, of, al, qaeda, leader, os...      YES\n",
      "4484  [On, the, 35th, anniversari, of, the, first, l...       NO\n",
      "4485  [test, on, anim, show, they, could, get, throu...  UNKNOWN\n",
      "4486  [napster, which, start, as, an, unauthor, song...       NO\n",
      "4487  [the, organ, of, the, 15th, intern, aid, confe...  UNKNOWN\n",
      "4488  [the, u.s., ambassador, to, manila, franci, ri...      YES\n",
      "4489  [everi, year, 40,000, babi, are, born, with, d...      YES\n",
      "4490  [A, former, petti, thief, who, convert, and, f...       NO\n",
      "4491  [one, of, michigan, 's, largest, newspap, chai...      YES\n",
      "4492  [the, new, arriv, follow, a, similar, number, ...      YES\n",
      "4493  [fighter, loyal, to, moqtada, al-sadr, shot, d...       NO\n",
      "4494  [continu, it, buy, spree, ibm, said, wednesday...      YES\n",
      "\n",
      "[4495 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nomFichier = \"dataset_complet\"\n",
    "    extension = \".csv\"\n",
    "    path = \"data\"\n",
    "\n",
    "\n",
    "    if not os.path.isfile(nomFichier + extension):\n",
    "        concatDataset(path,nomFichier)\n",
    "        contenu = ouvrirFichier(nomFichier + \".txt\")\n",
    "        dataFrame = transformeFileData(contenu,nomFichier)\n",
    "\n",
    "    else :\n",
    "        dataFrame = pd.read_csv(nomFichier + extension) \n",
    "\n",
    "    dataFrameTokenise = tokenization(dataFrame)\n",
    "    print(dataFrameTokenise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
